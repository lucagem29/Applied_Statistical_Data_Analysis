== Prompt Engineering ==

=== Introduction ===

Prompt engineering has been on everyone's lips for quite a while now. Especially the success of OpenAI's ChatGPT and other Large Language Models further emphasizes the relevance of the topic.
In this article we will deal with the basics of a highly diverse topic, always considering practical approaches towards the application of prompt engineering in data science and python. 
Prompt Engineering will help you to debug your code fast and efficiently, generate stunning reports quickly and plan your strategy in a more structured way.
Mastering prompt engineering can lead to an unprecendented increase of productivity in your work with python. So, let us dive in. 

=== Prerequisites ===

As this is a practical topics, readers should already have tested some LLM-based tools like ChatGPT in order to experience what these tools are capable of and where their boundaries lie. 
Furthermore, a basic understanding of Python is advised aswell as some experience with Jupyter Notebooks. Especially users should know, where to double-check the output of the used tools. 

=== How it works ===

==== Principles of Large Language Modules ====

The core of tools like ChatGPT are Large Language Models (LLMs). These models belong to the class of deep learning algorithms and are trained with an enormous quanitity of data. 
LLMs can also be viewed as an outstanding example of generative artifical intelligence. Therefore LLMs are able to generate (new) content based on input (prompts) consisting of human language. 
Any attemptst to explain generative AI and deep learning in only a few sentences are probably doomed to fail. Hence, we will focus on the basic concept regarding the way an LLM interprets and used the given input. 
 https://aws.amazon.com/what-is/large-language-model/ 

Very simply put, the LLM learns the distribution, relationships and concepts related to words based on huge amounts of text and unsupervised machine learning approaches. 
Hence, the modell is later able to predict the next words in a sentence or generate new text based on the input.
https://blogs.nvidia.com/blog/what-are-large-language-models-used-for/

However, the way LLMs interpret input is not comparable to the way the human brains does. A crucial step in the input-interpretation and output-generation of an LLM is Tokenization.
Tokenization refers to the process of breaking down text input into small units which can then be represented as numbers. Those numerical interpretations are then fed into the model. The model evaluates the input and creates an answer, which is then detokenized and put out as human language. 
https://docs.mistral.ai/guides/tokenization/

One can already see that the way an LLM interprets input is very structured and based on the way the model was trained. This is why prompt engineering can be helpful to guarantee better and more relevant outcomes while working with LLM-based tools.

==== Best practices and types of prompts ====

Since the user of an LLM-based tool is not able to see behind the curtain of the LLM-tool, one should be advised that prompt engineering is often an iterative process. Also prompt engineering is a relatively new idea, since it only became relevant when people started using tools like ChatGPT. 
Therefore prompt engineering is a highly dynamic field and best practices are still evolving.
Nevertheless, in the last years some basic approaches towards prompt engineering emerged.  https://aws.amazon.com/what-is/prompt-engineering/?nc1=h_ls

Generally speaking, a user should always state what outcome, piece of information is most important for the task at hand. A clear and logical structure of the prompt is also helpful.
Furthermore, it can be helpful to break down complex tasks into smaller and more simple prompts. 
In the following the most common types pf prompts and best practices will be briefly discussed

The simplest kind of prompt is the so-called *zero-shot prompt*. It consists of a simple question or instruction with no additional context. More advanced but still a zero-shot prompt is the assignment of a "role" to the model.
Assigning a role is often helpful, since the model will act as if it was a person with a specific professional background or opinion. One example of an zero-shot prompt:

"Act as a professional travel-planner. Next week I will be spending a few days in Paris. Plan a 2-day trip for me."

Another technique involves the usage of examples. Such approaches are called *one-, few- and multi-shot prompts*. A one-shot prompt consists of one example, a few-shot prompt of a few examples and a multi-shot prompt of many examples.
This technique is helpful if you want the model to replicate a pattern or when the output has to be structured in a very specific way, which is often the case if the output has to be scalable or replicable.
One example for a simple multi-shot prompt could be:

"Your task is to categorize customer reviews into the categories positive, neutral or negative. Here are some examples:
1. Statement: "The product exceeded my expectations. 
   Category: Positive
2. Statement: "The product was not as good as I expected.
    Category: Negative
3. Statement: "The product was okay, but not great."
    Category: Neutral
4. Statement: "The product was a total waste of money."
    Category: Negative"

Please categorize this statement: "I sent the product back because it was broken"."

In order to combine human intelligence with the power of LLMs, so-called Chain-of-thought-prompts (CoT) are another crucial technique. These prompts are iterative and demand the model to explain it's reasoning.
https://developers.google.com/machine-learning/resources/prompt-eng

A very simple example of a CoT prompt could be:
"What could be a successful strategy for a company to increase it's revenue given the current market situation and a restricted budget? Think step by step."

Another concept called "scene defining" refers to the specification of a given context. This is helpful since it emphasized the most crucial components of the task at hand. One example for "scene definining" could be:
"Act as a CEO of an AI-startup, based in LÃ¼neburg, Germany. You are currently planning the next steps for an expansion strategy towards the US-market. An inital market-analysis has shown that Americans are specifically interested in AI-based solutions for the healthcare sector. What would be your next steps?"

If you want to play it really clever you can also use a LLM to generate, evaluate and improve your prompts. This idea is called "Prompt refinement". This might be especially helpful if you are not experienced in working with the respective model. 
Such as prompt could look like this:
"Act as an expert in prompt engineering. I am using a image-generation tool called "Midjourney" and I want to generate a picture of a cat. Please generate a prompt for me." 
https://www.jmir.org/2024/1/e60501 

==== Prompt Engineering for Python ====

Some tools like ChatGPT or Anthropic's Claude provide an outstanding opportunity to learn writing code and conducting data science tasks. 
Generally, the explained prompt-engineering-techniques also apply to writing code in python. There is not one specific to find high-qualitiy prompts with regard to python. Nevertheless, the following workflow and the prompting examples will be helpful to get a feeling for prompt engineering using python. 
Assume that you are responsible for the analysis of a dataset. Since analysing data is a complex task, you will not be able to finish a task in one prompt. 

Rather you would start by introducing the context and the topic of your task to your chosen LLM-tool.

Step 1: Scene defining (e.g. setting the context, assigning a role, specifiying the task and the desired outcome, add your prefered way of working)
Example: "Act as a data scientist with at least 5 years of experience and a professional background in [add topic with regard to your data set]. 
Our task is to analyse a dataset containing information about [add information about your dataset].
The desired output is a report containing python-code, visualizations and a summary of the most important insights. 
Focus on the following aspects: [add aspects you want to focus on].
I will ask you specific questions and give you instructions on how to proceed and you will generate the python code for that particular step." 

Now the LLM should be setup to assist you in the analysis of the dataset. It also can assist with idea generation for interpretation of the data, since we assigned it to be trained in the topic of specific dataset. 

In the next step you could upload the dataset to the LLM-tool (always consider privacy issues before you do that). In that case the LLM knows all the names of variables and the structure of the data.
Assume that you have specific idea for an analysis (e.g. you want to do a group comparison, or you want to build a generalized linear model etc.)
Altenatively you could ask the LLM for specific idea which kind of analysis would be appropriate for the dataset.

Step 2: Prompt refinement (e.g. generating a specific analysis idea)
Example: "I want to build a generalized linear model to predict the variable [add variable name]. Please explain the workflow for such an analysis and explain the reasons behind each step." 

The LLM-tool will then come up with an outline for your analysis. You can then ask for the python code for each step.
Since it is often not obvious why the LLM came up with a specific answer or code in this case, one should always advise the LLM to document and explain the code clearly. 
With respect to the techniques we defined earlier, it can be very helpful to use examples and a multi-shot prompt in this case. For instance, if you found a code-snippet in the internet you could use that as an outline for the code the LLM should generate. 

Step 3: Multi-shot prompt (e.g. generating the code for the analysis)
Example: "Please generate the code for [step 1 of the analysis you defined earlier]. Here is an example of code I found on the internet: [add code-snippet]. Use this snippet as an outline for our code and adapt it to our dataset and variables. Also always explain
the code clearly and document each step in the code." 

From that on you can continue with each step of the analysis in the same way. You can always test the code the LLM generated simultaneously in a Jupyter Notebook. If an error occurs you can ask the LLM to debug and specify the error-message you received.

Step 4: Debugging (e.g. by asking for the reasons behind an error message)
Example: "I received the following error message in this step: [add error message]. Please debug the code and explain the reasons for the error."

The most crucial part in prompt engineering for python is viewing the LLM as an enhancement to your work and not as an replacement of critical thinking. 
The proposed workflow is iterative and focused around the reasoning of the LLM and therefore enables the user to doublecheck the generated output with reliable sources such as the documentations of the respective python-libraries.
Therefore the interaction between the user and the LLM amples to oppurtunity to learn and improve your coding skills while already producing high-qualitiy data analysis in python. 

=== Opportunities and risks ===

Prompt engineering opens up a wide range of opportunities for data scientists and python coders.
The generation of code is fast, efficient and often of high quality. The LLM-models can also assist in debugging code, analzying error or even help you analyse results of your analysis.
Good prompts will help you define a strategy for your analysis, plan the execution of your workflow and explain steps you do not understand. 
In general prompt engineering enables users to achieve a level of knowledge and quality that might be very difficult to obtain for a beginner in python. 
Furthermore and probably even more important, prompt engineering can serve as a cornerstone in learning data science and data analysis. Iterating on the output of the LLM, comparing it with other trusted sources, discovering errors and improvements is a great strategy to learn fast and generate adequate results in the maintime. 
For more advanced python users prompt engineering is also a tools for automating repetitive tasks. Since LLM can be directly integrated in python workflows trough an API, users can completely automate specific parts of the analysis and therefore scale workflows. 
Also so called "custom-gpts" , which essentially are mini-gpts trained for a very specific task, can be used to generate a highly specified output. This output can then be used as a variable by other tools and platforms trough APIs again.
Hence, prompt engineering is a powerful domain for python beginners aswell as advanced adopters, trying to scale their workflows. 
In a broader point of view, prompt engineering contributes to the democratization of AI and data science, enabling more and more people to work on meaningful projects.

However, prompt engineering is also associated with risks.
Crucially, the LLM works behind a curtain. If the user tests the code iteratively in a Jupyter Notebook, one could at least determine which code-snippet throws an error. 
But without a deep understanding of the code and the used statistical methods, you are forced to blindly trust the LLM. This implies the risks that the generated code will either not work or is very inefficient. 
In such cases, debugging the code can become a total nightmare. Especially if the user does not understand the code and the generated code-snippet is complex and long.
Therefore even great prompt engineering might be not enough to guarantee high-quality results.
In addition, prompt engineering might lead to a fatigue in learning the underlying python concepts and statistical methods. Some users might feel like, it is not neccessary to double-check the generated code and will therefore not learn and remain on a rather low professional level. 
Also even prompts that consider a variety of different prompting techniques might not work out as expected. There is still variety in the generated output, even if you are using the same prompt. Of course the generated code will also differ depending on which model or tool you use, since not every model is optimized for coding tasks. 
Furthermore the iteration on prompts can lead to a high-quality prompt for that one specific model. It is not guaranteed that this exact prompt yield good results when used in other models. 
In a nutshell, prompt engineering might lead to a loss in the users critical thinking and problem-solving ability, therefore becoming a worse python-coder. 

=== Normativity ===

As always when working with artifical intelligence, the question of ethics arrise. Prompt engineering is no exception. 
Ethical considerations regarding prompt engineering become clear when we think about the usage of examples for enhancing our prompts. 
As explained using examples is a very efficient way to structure the output of the LLM and ensure a more adequate code. Here the danger of using private data (e.g. names, adresses, confidential information etc.) arrises. Even though a user can normally specify that their respective input data (e.g. what they type in ChatGPT) should not be used for training the model, it is not clear in every case what the respective company behind the LLM does with the data. 
Sharing private information about yourself or someone else in primpting examples can in the worst case lead to a data breach and vulnerable damage to the respective person. Also, such offenses can lead to legal consequences and severe fines. 
Also, the user must be sure that the provided code is not protected under any license and can be used for the intended purpose.
Another reason for the danger of using examples might be a bias in the examples a user provides. Normally the user will not be able to provide a vast amount of examples in one prompt anyways, nonetheless the input can carry a sensitive bias which will be reproduces in the generated output.  
This might seem arbitrary when (only) generating code, but if you prompt the LLM to analyze and identify patterns in your code's outputs, the bias can be very harmful. It should be mentioned that any use of artifical intelligence implies the risks of carrying a bias, since the model might be trained with biased data. This is an inherent risks you can not mitigate using prompt engineering. 
Prompt engineering makes coding more accessable for everyone, including people that might have dark intentions. As a technique prompt engineering is neither good or bad, such as artificla intelligence itself. Nevertheless, it should be mentioned that prompt engineering might enable people to generate code for malicious purposes. 
Since prompt engineering becomes more an more common it might lead to an intransparency in research or work environments. If code is shared online, users might not be able anymore to differentiate between human written code and code that was written by or under the influence of an LLM. A normative approach to this issue might be the clear labeling of code that was generated by an LLM aswell as sharing the used prompt along with the code. 
Last but not least, prompt engineering is a powerful tools. Also, most LLM-tools can even be embedded in automatic workflows trough the use of APIs. Therefore prompt engineering will contribute to shifts ins the labour market, leaving people in repetitive occoputations without a job.
Some of theses issues can definitely not be specifically tailored towards prompt engineering, but are rather general issues regarding the usage of artifical intelligence. Nevertheless prompt engineering opens the door to use human language to interact with high complex and very developed AI-tools. Their role in possible negative consequences regarding an omnipotent usage of AI should not be underestimated.

=== Outlook ===

As mentioned earlier, prompt engineering is a highly dynamic field. Research on specific prompt-engineering-techniques is being done in different domains.
There are already approaches to use artificial intelligence to automatically suggest prompts while you are writing them, tailoring them to your specific needs and developed with a deep understanding of the respective model.
So in the future we might not be forced to know high-quality prompting techniques, as the respective model will suggest them to us.
In addition there are also large tech-companies like OpenAI compiling databases of high-quality prompts and examples. Those different databases and guidelines could be used to develop prompting-standards similar to coding-standards like PEP8 for python.
Another interesting area could be the development of prompt engineering with regard to voice interaction between users and a LLM, since normally good prompts are highly structured which can be an issue in a natural voice interaction, where the user might not be able to structure the prompt in a way that the LLM understands it.
Furthermore we will probably see even more LLMs embedded in automated workflows, with highly specified tasks.  

=== Further reading ===

Since examples might be helpful for understanding the tweaks of prompt engineering, here is a list of prompt engineering guides and examples from key-players in the AI-industry for you refine your knowledge. 

https://platform.openai.com/docs/guides/prompt-engineering
https://docs.mistral.ai/guides/prompting_capabilities/
https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview#prompting-vs-finetuning 
https://cloud.google.com/discover/what-is-prompt-engineering?hl=en
https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-engineering-guidelines.html 

For any advanced python user, this documentations might be helpful, when integrating and prompt engineering LLMs in your workflows. 
https://platform.openai.com/docs/overview - shows all you need to know about OpenAI's API and how to use it.
https://www.make.com/en/help/app/openai-dall-e-chatgpt - make is a no-code or low-code platform that makes you integrate LLMs in your workflow, but it is aimed at coding-beginners with a very simple interface. 


=== References ===

=== Author and date: ===

Luca Gemballa, 11.12.2024. 